# PyTorch QAT to Quantized TFLite Conversion Example

This project demonstrates a pipeline to convert a PyTorch model trained with Quantization-Aware Training (QAT) into a fully quantized INT8 TFLite model using ONNX as an intermediate format.

## Pipeline Overview

The conversion process implemented in `main.py` follows these steps:

1.  **PyTorch QAT Model:** Define a standard PyTorch model (`nn.Module`) incorporating `torch.quantization.QuantStub` and `torch.quantization.DeQuantStub` at the input and output respectively.
2.  **PyTorch QAT Process:** Apply PyTorch's native QAT workflow:
    *   Set the quantization backend (e.g., `qnnpack`).
    *   Configure QAT (`get_default_qat_qconfig`).
    *   Prepare the model using `torch.quantization.prepare_qat`.
    *   Perform calibration passes or fine-tuning on the prepared model.
    *   Convert the model to a quantized INT8 PyTorch model using `torch.quantization.convert`.
3.  **Export to ONNX:** Export the resulting INT8 PyTorch model to the ONNX format using `torch.onnx.export`.
4.  **ONNX to TF SavedModel (`onnx2tf`):** Use the `onnx2tf` command-line tool with the `-oi8` flag. This step converts the `.onnx` file into an INT8-ready TensorFlow SavedModel.
    *   *Note:* As observed during development (with `onnx2tf==1.27.2`), this tool may ignore the `-o` output path argument for the SavedModel when `-oi8` is used, instead creating the SavedModel in an `i8` directory in the current working directory. The script accounts for this.
    *   `onnx2tf` also generates FP16 and FP32 `.tflite` files during this step, which are moved to the main output folder by the script.
5.  **SavedModel to INT8 TFLite (`TFLiteConverter`):** Use TensorFlow Lite's Python `TFLiteConverter` to perform the final conversion:
    *   Load the SavedModel generated by `onnx2tf` (from the `i8` directory).
    *   Provide a `representative_dataset`: A generator function yielding sample floating-point input data is required for the converter to determine the quantization parameters (scale, zero-point) for activations.
    *   Set optimizations and target specifications for full INT8 quantization (`optimizations = [tf.lite.Optimize.DEFAULT]`, `inference_input_type = tf.int8`, `inference_output_type = tf.int8`, `target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]`).
    *   Convert the model to the final `.tflite` format.

## Dependencies

Run 
```bash 
  uv venv --python=python3.10
  source ./venv/bin/activate
  uv pip install -e .
```

The core dependencies required (see `pyproject.toml`):

*   `torch`
*   `onnx`
*   `onnx2tf` (and its own dependencies like `onnx-graphsurgeon`, `psutil`, `ai-edge-litert`, `sng4onnx`)
*   `tensorflow` (specifically for `tf.lite`)
*   `numpy<2.0` (pinned due to TensorFlow 2.15 compatibility)
*   `tf-keras`
*   `tensorflow-probability`

## How to Run

Run
```bash
  python main.py
```

## Output

The script will create an output folder (default: `conversion_output`). This folder will contain:

*   `quant_qat.onnx`: The intermediate quantized ONNX model.
*   `quant_qat_float16.tflite`: FP16 TFLite model (generated by `onnx2tf`).
*   `quant_qat_float32.tflite`: FP32 TFLite model (generated by `onnx2tf`).
*   `quant_qat_int8.tflite`: The final fully quantized INT8 TFLite model.

Additionally, an `i8` directory containing the intermediate TensorFlow SavedModel generated by `onnx2tf` will be created in the project's root directory.